{"paragraphs":[{"text":"%md\n\n# Chargement des données dans S3","user":"anonymous","dateUpdated":"2020-02-04T16:22:54+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Chargement des données dans S3</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1580833361161_-1506648028","id":"20200204-162241_156696498","dateCreated":"2020-02-04T16:22:41+0000","dateStarted":"2020-02-04T16:22:54+0000","dateFinished":"2020-02-04T16:22:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4227"},{"title":"Variable à changer","text":"import com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicSessionCredentials\n    \n\n\nval bucketname =\"Your_bucket\"\n\nval AWS_ID=\"Your_ID\"\nval AWS_KEY=\"Your_Key\"\nval AWS_TOKEN=\"Your_Token_if_needed_because_Educate\"\n\n\n// la classe AmazonS3Client n'est pas serializable\n// on rajoute l'annotation @transient pour dire a Spark de ne pas essayer de serialiser cette classe et l'envoyer aux executeurs\n@transient val awsClient = new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY, AWS_TOKEN))\n\nsc.hadoopConfiguration.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\nsc.hadoopConfiguration.set(\"fs.s3a.access.key\", AWS_ID) // mettre votre ID du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.secret.key\", AWS_KEY) // mettre votre secret du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.session.token\", AWS_TOKEN)","user":"anonymous","dateUpdated":"2020-02-04T16:25:35+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one deprecation warning; re-run with -deprecation for details\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicSessionCredentials\nbucketname: String = Your_bucket\nAWS_ID: String = Your_ID\nAWS_KEY: String = Your_Key\nAWS_TOKEN: String = Your_Token_if_needed_because_Educate\nawsClient: com.amazonaws.services.s3.AmazonS3Client = com.amazonaws.services.s3.AmazonS3Client@7bbd8a2b\n"}]},"apps":[],"jobName":"paragraph_1580832749155_1432666031","id":"20200122-185347_1765322798","dateCreated":"2020-02-04T16:12:29+0000","dateStarted":"2020-02-04T16:25:35+0000","dateFinished":"2020-02-04T16:25:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4228"},{"title":"Fonction de téléchargement","text":"import sys.process._\nimport java.net.URL\nimport java.io.File\nimport java.io.File\nimport java.nio.file.{Files, StandardCopyOption}\nimport java.net.HttpURLConnection \nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types.IntegerType\n\ndef fileDownloader(urlOfFileToDownload: String, fileName: String) = {\n    val url = new URL(urlOfFileToDownload)\n    val connection = url.openConnection().asInstanceOf[HttpURLConnection]\n    connection.setConnectTimeout(5000)\n    connection.setReadTimeout(5000)\n    connection.connect()\n\n    if (connection.getResponseCode >= 400)\n        println(\"error\")\n    else\n        url #> new File(fileName) !!\n}","user":"anonymous","dateUpdated":"2020-02-04T16:12:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import sys.process._\nimport java.net.URL\nimport java.io.File\nimport java.io.File\nimport java.nio.file.{Files, StandardCopyOption}\nimport java.net.HttpURLConnection\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types.IntegerType\nwarning: there was one feature warning; re-run with -feature for details\nfileDownloader: (urlOfFileToDownload: String, fileName: String)Any\n"}]},"apps":[],"jobName":"paragraph_1580832749160_-1524441353","id":"20200122-185434_689366639","dateCreated":"2020-02-04T16:12:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4229"},{"title":"Téléchargement du fichier d'indexation","text":"fileDownloader(\"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\", \"/tmp/masterfilelist.txt\") // save the list file to the Spark Master\nfileDownloader(\"http://data.gdeltproject.org/gdeltv2/masterfilelist-translation.txt\", \"/tmp/masterfilelist_translation.txt\") ","user":"anonymous","dateUpdated":"2020-02-04T16:12:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res12: Any = \"\"\nres13: Any = \"\"\n"}]},"apps":[],"jobName":"paragraph_1580832749160_1664674833","id":"20200122-185517_1865818114","dateCreated":"2020-02-04T16:12:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4230"},{"title":"Transfert du fichier vers Bucket S3","text":"awsClient.putObject(bucketname, \"masterfilelist_translation.txt\", new File( \"/tmp/masterfilelist_translation.txt\") )\nawsClient.putObject(bucketname, \"masterfilelist.txt\", new File( \"/tmp/masterfilelist.txt\") )\n\n\nimport org.apache.spark.sql.SQLContext\n\nval sqlContext = new SQLContext(sc)\nval filesDF = sqlContext.read.\n                    option(\"delimiter\",\" \").\n                    option(\"infer_schema\",\"true\").\n                    csv(\"s3://\" + bucketname + \"/masterfilelist.txt\").\n                    withColumnRenamed(\"_c0\",\"size\").\n                    withColumnRenamed(\"_c1\",\"hash\").\n                    withColumnRenamed(\"_c2\",\"url\").\n                    cache\n\n\nval filesDF_translation = sqlContext.read.\n                    option(\"delimiter\",\" \").\n                    option(\"infer_schema\",\"true\").\n                    csv(\"s3://\" + bucketname + \"/masterfilelist_translation.txt\").\n                    withColumnRenamed(\"_c0\",\"size\").\n                    withColumnRenamed(\"_c1\",\"hash\").\n                    withColumnRenamed(\"_c2\",\"url\").\n                    cache","user":"anonymous","dateUpdated":"2020-02-04T16:12:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res15: com.amazonaws.services.s3.model.PutObjectResult = com.amazonaws.services.s3.model.PutObjectResult@61a07a11\nres16: com.amazonaws.services.s3.model.PutObjectResult = com.amazonaws.services.s3.model.PutObjectResult@5e2013b3\nimport org.apache.spark.sql.SQLContext\nwarning: there was one deprecation warning; re-run with -deprecation for details\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@66c8dafc\nfilesDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [size: string, hash: string ... 1 more field]\nfilesDF_translation: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [size: string, hash: string ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1580832749160_-1295030511","id":"20200122-185632_1196189728","dateCreated":"2020-02-04T16:12:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4231"},{"title":"Chargement des fichiers en fonction des dates","text":"val d_range = \"201906\"\n// --\nval sampleDF = filesDF.filter(col(\"url\").contains(\"/\" + d_range)).cache\nval sample_DF_translation = filesDF_translation.filter(col(\"url\").contains(\"/\" + d_range)).cache\n\n\nobject AwsClient{\n    val s3 = new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY,AWS_TOKEN))\n}\n\n\nsampleDF.select(\"url\").repartition(100).foreach( r=> {\n            val URL = r.getAs[String](0)\n            val fileName = r.getAs[String](0).split(\"/\").last\n            val dir = \"/tmp/\"\n            val localFileName = dir + fileName\n            fileDownloader(URL,  localFileName)\n            val localFile = new File(localFileName)\n            AwsClient.s3.putObject(bucketname, fileName, localFile )\n            localFile.delete()\n            \n})\n\n\nsample_DF_translation.select(\"url\").repartition(100).foreach( r=> {\n            val URL = r.getAs[String](0)\n            val fileName = r.getAs[String](0).split(\"/\").last\n            val dir = \"/tmp/\"\n            val localFileName = dir + fileName\n            fileDownloader(URL,  localFileName)\n            val localFile = new File(localFileName)\n            AwsClient.s3.putObject(bucketname, fileName, localFile )\n            localFile.delete()\n            \n})\n","user":"anonymous","dateUpdated":"2020-02-04T16:12:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"d_range: String = 201906\nsampleDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [size: string, hash: string ... 1 more field]\nsample_DF_translation: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [size: string, hash: string ... 1 more field]\nwarning: there was one deprecation warning; re-run with -deprecation for details\ndefined object AwsClient\n"}]},"apps":[],"jobName":"paragraph_1580832749161_446850170","id":"20200122-185723_15013232","dateCreated":"2020-02-04T16:12:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4232"},{"user":"anonymous","dateUpdated":"2020-02-04T16:12:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1580832749161_1352773838","id":"20200124-053702_902790354","dateCreated":"2020-02-04T16:12:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4233"}],"name":"Data importation to S3","id":"2F2AW9MFG","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}